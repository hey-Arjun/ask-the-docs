# ğŸ“„ Ask-the-Docs â€” RAG Based Document Question Answering
A full-stack **Retrieval-Augmented Generation (RAG)** system that allows users to upload documents (**PDF / TXT**) and ask natural-language questions grounded strictly in the uploaded content.

This project demonstrates **end-to-end AI system engineering**, covering backend RAG pipelines, UI integration, Dockerization, and real-world AWS deployment using **Nginx reverse proxy**.

---

## ğŸ§  What This Project Demonstrates

- Retrieval-Augmented Generation (RAG)
- Vector databases (FAISS)
- Semantic search using embeddings
- Context-grounded LLM answering
- Backend + UI separation
# ğŸ“„ Ask-the-Docs â€” RAG Based Document Question Answering System

A full-stack **Retrieval-Augmented Generation (RAG)** system that allows users to upload documents (**PDF / TXT**) and ask natural-language questions grounded strictly in the uploaded content.

This project demonstrates **end-to-end AI system engineering**, covering backend RAG pipelines, UI integration, Dockerization, and real-world AWS deployment using **Nginx reverse proxy**.

---

## ğŸ§  What This Project Demonstrates

- Retrieval-Augmented Generation (RAG)
- Vector databases (FAISS)
- Semantic search using embeddings
- Context-grounded LLM answering
- Backend + UI separation
- Dockerized microservices
- Cloud deployment (AWS EC2)
- Reverse proxy with Nginx
- Production-style architecture

---

## ğŸ” Core Features

### âœ… Document Upload
- Supports **PDF** and **TXT**
- Automatic OCR fallback for scanned PDFs
- Large file upload support via Nginx configuration

### âœ… RAG Pipeline
- Recursive text chunking
- Semantic embeddings generation
- FAISS vector similarity search
- Top-k context retrieval
- Strict context-grounded answers

### âœ… Accurate Answers
- LLM answers strictly from retrieved document context
- Prevents hallucinations
- Clear fallback when answer is not found

### âœ… Source Tracking
- Retrieved document chunks are logged
- Answer sources stored for explainability

### âœ… Interaction Logging
- Every question and answer stored in:

- Includes timestamp, file name, question, answer, and sources

---

## ğŸ§  RAG Architecture (Backend)

### 1ï¸âƒ£ Document Loading
- PDFs parsed using **Poppler**
- OCR fallback using **Tesseract**
- Text normalized and cleaned

### 2ï¸âƒ£ Chunking
- RecursiveCharacterTextSplitter
- Overlapping chunks for context continuity

### 3ï¸âƒ£ Embeddings
- Text embeddings generated for each chunk
- Stored in FAISS vector index

### 4ï¸âƒ£ Retrieval
- User query embedded
- Top-K most relevant chunks retrieved
- Context dynamically assembled

### 5ï¸âƒ£ Answer Generation
- Prompt constructed using:
  - retrieved context
  - user question
- LLM generates final grounded response

---

## âš™ï¸ Tech Stack

### Backend
- FastAPI
- Python 3.11
- FAISS
- OpenAI / LLM API
- Tesseract OCR
- Poppler Utils

### Frontend
- Gradio
- Clean vertical user interface
- File upload and Q&A workflow

### Infrastructure
- Docker
- AWS EC2
- Nginx Reverse Proxy

---

## ğŸ³ Docker Architecture

### Backend Container
- Runs FastAPI application
- Exposed internally on port **8000**
- Accessible via Nginx route `/api`

### UI Container
- Runs Gradio interface
- Exposed internally on port **7860**
- Served publicly via Nginx root `/`

### Benefits
- Clean separation of concerns
- Independent service scaling
- Environment isolation
- Production-ready deployment

---

## ğŸŒ Nginx Reverse Proxy

Nginx handles:

- Public access on port **80**
- Routing between UI and backend
- Large file upload handling
- Security boundary between internet and containers

---

## â˜ï¸ AWS Deployment

### Platform
- AWS EC2 (Amazon Linux 2023)
- Free Tier compatible

### Deployment Flow
1. Build Docker images locally  
2. Push images to Docker Hub  
3. Pull images on EC2 instance  
4. Run containers  
5. Configure Nginx reverse proxy  
6. Expose public IP for browser access  

---

## ğŸ“ Project Structure

ask-the-docs/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ rag/
â”‚ â”‚ â”œâ”€â”€ loader.py
â”‚ â”‚ â”œâ”€â”€ chunker.py
â”‚ â”‚ â”œâ”€â”€ embeddings.py
â”‚ â”‚ â”œâ”€â”€ vector_store.py
â”‚ â”‚ â”œâ”€â”€ retriever.py
â”‚ â”‚ â””â”€â”€ prompt.py
â”‚ â”œâ”€â”€ llm/
â”‚ â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ ui/
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ history.json
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md



---

## ğŸš€ Future Improvements

- Multi-user session isolation
- Persistent vector storage
- Chat-style UI
- Streaming responses
- Authentication
- HTTPS and custom domain
- Docker Compose orchestration



- Dockerized microservices
- Cloud deployment (AWS EC2)
- Reverse proxy with Nginx
- Production-style architecture

---

## ğŸ” Core Features

### âœ… Document Upload
- Supports **PDF** and **TXT**
- Automatic OCR fallback for scanned PDFs
- Large file upload support via Nginx configuration

### âœ… RAG Pipeline
- Recursive text chunking
- Semantic embeddings generation
- FAISS vector similarity search
- Top-k context retrieval
- Strict context-grounded answers

### âœ… Accurate Answers
- LLM answers strictly from retrieved document context
- Prevents hallucinations
- Clear fallback when answer is not found

### âœ… Source Tracking
- Retrieved document chunks are logged
- Answer sources stored for explainability

### âœ… Interaction Logging
- Every question and answer stored in:

- Includes timestamp, file name, question, answer, and sources

---

## ğŸ§  RAG Architecture (Backend)

### 1ï¸âƒ£ Document Loading
- PDFs parsed using **Poppler**
- OCR fallback using **Tesseract**
- Text normalized and cleaned

### 2ï¸âƒ£ Chunking
- RecursiveCharacterTextSplitter
- Overlapping chunks for context continuity

### 3ï¸âƒ£ Embeddings
- Text embeddings generated for each chunk
- Stored in FAISS vector index

### 4ï¸âƒ£ Retrieval
- User query embedded
- Top-K most relevant chunks retrieved
- Context dynamically assembled

### 5ï¸âƒ£ Answer Generation
- Prompt constructed using:
  - retrieved context
  - user question
- LLM generates final grounded response

---

## âš™ï¸ Tech Stack

### Backend
- FastAPI
- Python 3.11
- FAISS
- OpenAI / LLM API
- Tesseract OCR
- Poppler Utils

### Frontend
- Gradio
- Clean vertical user interface
- File upload and Q&A workflow

### Infrastructure
- Docker
- AWS EC2
- Nginx Reverse Proxy

---

## ğŸ³ Docker Architecture

### Backend Container
- Runs FastAPI application
- Exposed internally on port **8000**
- Accessible via Nginx route `/api`

### UI Container
- Runs Gradio interface
- Exposed internally on port **7860**
- Served publicly via Nginx root `/`

### Benefits
- Clean separation of concerns
- Independent service scaling
- Environment isolation
- Production-ready deployment

---

## ğŸŒ Nginx Reverse Proxy

Nginx handles:

- Public access on port **80**
- Routing between UI and backend
- Large file upload handling
- Security boundary between internet and containers

---

## â˜ï¸ AWS Deployment

### Platform
- AWS EC2 (Amazon Linux 2023)
- Free Tier compatible

### Deployment Flow
1. Build Docker images locally  
2. Push images to Docker Hub  
3. Pull images on EC2 instance  
4. Run containers  
5. Configure Nginx reverse proxy  
6. Expose public IP for browser access  

---


## ğŸš€ Future Improvements

- Multi-user session isolation
- Persistent vector storage
- Chat-style UI
- Streaming responses
- Authentication
- HTTPS and custom domain
- Docker Compose orchestration



